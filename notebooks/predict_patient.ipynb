{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c245d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression_Prob: mean=0.000, min=0.00, max=0.00\n",
      "SVM (RBF)_Prob: mean=0.730, min=0.73, max=0.73\n",
      "XGBoost_Prob: mean=0.425, min=0.34, max=0.47\n",
      "Neural Net_Prob: mean=1.000, min=1.00, max=1.00\n",
      "Transformer_Prob: mean=0.578, min=0.58, max=0.58\n"
     ]
    }
   ],
   "source": [
    "for col in pred_df.columns:\n",
    "    if col.endswith(\"_Prob\"):\n",
    "        print(f\"{col}: mean={pred_df[col].mean():.3f}, min={pred_df[col].min():.2f}, max={pred_df[col].max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aac1bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ Dropping non-numeric columns: ['source']\n",
      "âœ… Scaler fitted on shape: (916, 13)\n",
      "âœ… Loaded 5 new patients\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for HeartTransformer:\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 139\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Initialize and load trained weights\u001b[39;00m\n\u001b[0;32m    138\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m HeartTransformer(\u001b[38;5;28mlen\u001b[39m(feature_names))\n\u001b[1;32m--> 139\u001b[0m \u001b[43mtransformer_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m transformer_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Prepare input tensor\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\CardioVescular_Disease_prediction\\CardioVescularDisease\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for HeartTransformer:\n\tsize mismatch for encoder.layers.0.linear1.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.layers.0.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.0.linear2.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 128]).\n\tsize mismatch for encoder.layers.1.linear1.weight: copying a param with shape torch.Size([2048, 64]) from checkpoint, the shape in current model is torch.Size([128, 64]).\n\tsize mismatch for encoder.layers.1.linear1.bias: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for encoder.layers.1.linear2.weight: copying a param with shape torch.Size([64, 2048]) from checkpoint, the shape in current model is torch.Size([64, 128])."
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ðŸ©º Batch Prediction â€” Clean, Warning-Free & Scaled\n",
    "# ================================================================\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === Paths ===\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "RAW_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, \"data\", \"processed\")\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"experiments\", \"results\")\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"data\", \"predictions\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === Load combined dataset (for fitting scaler) ===\n",
    "dataset_name = \"combined_uci_heart\"\n",
    "X = joblib.load(os.path.join(PROCESSED_DIR, f\"{dataset_name}_X.joblib\"))\n",
    "y = joblib.load(os.path.join(PROCESSED_DIR, f\"{dataset_name}_y.joblib\"))\n",
    "\n",
    "# Convert sparse to dense if needed\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X = X.toarray()\n",
    "\n",
    "# Drop non-numeric columns (like 'source')\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    if non_numeric:\n",
    "        print(f\"ðŸ§¹ Dropping non-numeric columns: {non_numeric}\")\n",
    "        X = X.drop(columns=non_numeric)\n",
    "\n",
    "# === Fit StandardScaler ===\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "print(f\"âœ… Scaler fitted on shape: {X.shape}\")\n",
    "\n",
    "# === Load new patient data ===\n",
    "csv_path = os.path.join(RAW_DIR, \"new_patients.csv\")\n",
    "if not os.path.exists(csv_path):\n",
    "    raise FileNotFoundError(f\"âŒ Missing new patient file: {csv_path}\")\n",
    "\n",
    "new_patients = pd.read_csv(csv_path)\n",
    "print(f\"âœ… Loaded {len(new_patients)} new patients\")\n",
    "\n",
    "# Ensure correct feature order\n",
    "feature_names = [\n",
    "    \"age\", \"ca\", \"chol\", \"cp\", \"exang\", \"fbs\", \"oldpeak\",\n",
    "    \"restecg\", \"sex\", \"slope\", \"thal\", \"thalach\", \"trestbps\"\n",
    "]\n",
    "new_patients = new_patients[feature_names]\n",
    "new_scaled = pd.DataFrame(scaler.transform(new_patients), columns=feature_names)\n",
    "\n",
    "# === Predict with all models ===\n",
    "predictions = {}\n",
    "\n",
    "# --- Classical models ---\n",
    "model_files = {\n",
    "    \"Logistic Regression\": \"logistic_regression_combined_uci_heart_model.joblib\",\n",
    "    \"SVM (RBF)\": \"svm_combined_uci_heart_model.joblib\",\n",
    "    \"XGBoost\": \"xgboost_combined_uci_heart_model.joblib\",\n",
    "}\n",
    "\n",
    "for model_name, file in model_files.items():\n",
    "    path = os.path.join(RESULTS_DIR, file)\n",
    "    if os.path.exists(path):\n",
    "        model = joblib.load(path)\n",
    "        probs = model.predict_proba(new_scaled)[:, 1]\n",
    "        predictions[model_name] = probs\n",
    "    else:\n",
    "        print(f\"âš ï¸ Missing model: {model_name}\")\n",
    "\n",
    "# --- Neural Network ---\n",
    "nn_path = os.path.join(RESULTS_DIR, \"neural_network_combined_uci_heart_model.pt\")\n",
    "if os.path.exists(nn_path):\n",
    "    class HeartMLP(torch.nn.Module):\n",
    "        def __init__(self, input_dim):\n",
    "            super().__init__()\n",
    "            self.net = torch.nn.Sequential(\n",
    "                torch.nn.Linear(input_dim, 64),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Dropout(0.3),\n",
    "                torch.nn.Linear(64, 32),\n",
    "                torch.nn.ReLU(),\n",
    "                torch.nn.Linear(32, 1),\n",
    "                torch.nn.Sigmoid()\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "\n",
    "    nn_model = HeartMLP(len(feature_names))\n",
    "    nn_model.load_state_dict(torch.load(nn_path))\n",
    "    nn_model.eval()\n",
    "    x_tensor = torch.tensor(new_scaled.to_numpy(), dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        nn_probs = nn_model(x_tensor).numpy().flatten()\n",
    "    predictions[\"Neural Net\"] = nn_probs\n",
    "else:\n",
    "    print(\"âš ï¸ Missing Neural Net model\")\n",
    "\n",
    "# --- Transformer ---\n",
    "# --- Transformer (Fixed: Real Prediction Instead of Static Joblib) ---\n",
    "tr_model_path = os.path.join(RESULTS_DIR, \"transformer_combined_uci_heart_model.pt\")\n",
    "\n",
    "if os.path.exists(tr_model_path):\n",
    "    import torch.nn as nn\n",
    "\n",
    "    class HeartTransformer(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim=64, num_heads=2, num_layers=2, ff_dim=2048):\n",
    "            super().__init__()\n",
    "            self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=hidden_dim,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=ff_dim,  # 2048 matches your checkpoint\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "            self.fc = nn.Linear(hidden_dim, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Add a sequence dimension\n",
    "            x = x.unsqueeze(1)\n",
    "            x = self.input_proj(x)\n",
    "            x = self.encoder(x)\n",
    "            x = x.mean(dim=1)\n",
    "            return self.sigmoid(self.fc(x))\n",
    "\n",
    "    # Initialize and load trained weights\n",
    "    transformer_model = HeartTransformer(len(feature_names))\n",
    "    transformer_model.load_state_dict(torch.load(tr_model_path, map_location=\"cpu\"))\n",
    "    transformer_model.eval()\n",
    "\n",
    "    # Prepare input tensor\n",
    "    x_tensor = torch.tensor(new_scaled.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tr_probs = transformer_model(x_tensor).numpy().flatten()\n",
    "\n",
    "    predictions[\"Transformer\"] = tr_probs\n",
    "    print(\"âœ… Transformer model loaded and predictions computed successfully.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Missing Transformer model weights (.pt)\")\n",
    "\n",
    "\n",
    "# === Combine and label ===\n",
    "pred_df = new_patients.copy()\n",
    "for model, probs in predictions.items():\n",
    "    pred_df[f\"{model}_Prob\"] = probs\n",
    "    pred_df[f\"{model}_Pred\"] = (probs >= 0.5).astype(int)\n",
    "    pred_df[f\"{model}_Risk\"] = np.where(probs >= 0.5, \"High Risk\", \"Low Risk\")\n",
    "\n",
    "# === Majority vote consensus ===\n",
    "pred_cols = [c for c in pred_df.columns if c.endswith(\"_Pred\")]\n",
    "pred_df[\"Majority_Vote\"] = (pred_df[pred_cols].mean(axis=1) >= 0.5).astype(int)\n",
    "pred_df[\"Consensus_Risk\"] = np.where(pred_df[\"Majority_Vote\"] == 1, \"High Risk\", \"Low Risk\")\n",
    "\n",
    "# === Clinician-friendly summary ===\n",
    "pred_df[\"Summary\"] = np.where(\n",
    "    pred_df[\"Consensus_Risk\"] == \"High Risk\",\n",
    "    \"Predicted HIGH risk by majority of models â€” further cardiac testing recommended.\",\n",
    "    \"Predicted LOW risk by majority of models.\"\n",
    ")\n",
    "\n",
    "# === Save results ===\n",
    "save_path = os.path.join(OUTPUT_DIR, \"new_patient_predictions.csv\")\n",
    "pred_df.to_csv(save_path, index=False)\n",
    "print(f\"ðŸ’¾ Saved predictions to: {save_path}\")\n",
    "\n",
    "# === Visualization ===\n",
    "risk_summary = {\n",
    "    model: np.mean(pred_df[f\"{model}_Pred\"]) for model in predictions.keys()\n",
    "}\n",
    "risk_df = pd.DataFrame(list(risk_summary.items()), columns=[\"Model\", \"High_Risk_Rate\"])\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.barplot(data=risk_df, x=\"Model\", y=\"High_Risk_Rate\", palette=\"coolwarm\")\n",
    "plt.title(\"Proportion of Patients Classified as High Risk per Model\")\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=30)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "display(pred_df.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CardioVescularDisease",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
